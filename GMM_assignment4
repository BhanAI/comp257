#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Oct 15 10:22:39 2024

@author: bettyhan
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_olivetti_faces
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler

# 1. Retrieve and load the Olivetti faces dataset
faces = fetch_olivetti_faces()
X = faces.data
y = faces.target

# 2. Split the dataset into training, validation, and test sets using stratified sampling
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

# 3. Apply PCA on the training data, preserving 99% of the variance
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.fit_transform(X_val)

pca = PCA(n_components=0.99)
X_train_pca = pca.fit_transform(X_train_scaled)
X_val_pca = pca.transform(X_val_scaled)
X_val_pca.shape
# 4. Determine the most suitable covariance type for the dataset
cov_types = ['full', 'tied', 'diag', 'spherical']
best_aic = np.inf
best_gmm = None
for cov_type in cov_types:
    gmm = GaussianMixture(n_components=10, covariance_type=cov_type)
    gmm.fit(X_train_pca)
    aic = gmm.aic(X_train_pca)
    if aic < best_aic:
        best_aic = aic
        best_gmm = gmm

print(f'Best covariance type: {best_gmm.covariance_type}')

# 5. Determine the minimum number of clusters using AIC or BIC

n_components = range(1, 21)
models = [GaussianMixture(n, covariance_type=best_gmm.covariance_type, random_state=0)
          for n in n_components]
aics = [model.fit(X_train_pca).aic(X_train_pca) for model in models]
#6. plot result from 3 and 4
#Plot the explained variance
plt.figure(figsize=(8, 4))
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('PCA Explained Variance')
plt.grid()
plt.show()
#
plt.plot(n_components, aics)
plt.show()
##### It appears that around 8 components minimizes the AIC. fit this to the data and confirm that it has converged


# 6. Output hard clustering assignments for each instance

final_gmm = GaussianMixture(8, covariance_type='full', random_state=0)
final_gmm.fit(X_train_pca)
print(gmm.converged_)

hard_assignments = final_gmm.predict(X_train_pca)
print("Hard clustering assignments (first 10):", hard_assignments[:10])
X_train_pca.shape
# 7. Output soft clustering probabilities for each instance
soft_assignments = final_gmm.predict_proba(X_train_pca)
print("Soft clustering probabilities (first 10):", soft_assignments[:10])

# 8. Generate new faces
new_faces = final_gmm.sample(5)[0]

# Inverse transform to original space
new_faces_original = pca.inverse_transform(new_faces)

# Visualize the generated faces
fig, axes = plt.subplots(1, 5, figsize=(15, 5))
for ax, face in zip(axes, new_faces_original):
    ax.imshow(face.reshape(64, 64), cmap='gray')
    ax.axis('off')
plt.show()
#10. Modify some images (e.g., rotate, flip, darken)
from PIL import Image

# Example: Flip an image

image = Image.fromarray((X_test[0] * 255).reshape(64, 64)).convert('L')
flipped_image = image.transpose(Image.FLIP_LEFT_RIGHT)
#flipped_image.show()

image2 = Image.fromarray((X_test[0] * 255).reshape(64, 64)).rotate(15)
#image2.show()
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot
plt.imshow(flipped_image, cmap='gray')
plt.title('flipped Image')
plt.axis('off')

plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd subplot
plt.imshow(image2, cmap='gray')
plt.title('rotated Image')
plt.axis('off')

plt.tight_layout()
plt.show()
# Convert modified images back to numpy arrays
rotated_array = np.array(image2).flatten()/ 255.0
flipped_array = np.array(flipped_image).flatten()/ 255.0
# Create a set of anomalous images
anomalous_images = np.array([rotated_array, flipped_array])

#11. anomaly detection
normal_scores = final_gmm.score_samples(X_val_pca)
modified_image_pca = pca.transform(anomalous_images)  # Transform modified image
modified_score = gmm.score_samples(modified_image_pca)

print(f'Normal score: {normal_scores.mean()}')
print(f'Modified score: {modified_score.mean()}')
